

# model
ctx_len: 1024 # ===> increase T_MAX in model.py if your ctx_len > 1024
n_layer: 24
n_embd: 768
# 2 options: RWKV or RWKV-ffnPre
model_type: RWKV-ffnPre
# in case of fine-tuning: (not yet implemented)
model_pretrained_path: "/workspace/spiking_workspace/SpikeGPT-OpenWebText-216/SpikeGPT-216M.pth"
use_pretrained: False

# wandb
wandb_logging: False
wandb_project: "spiking_lm"
wandb_prefix: "spikeGPT"
wandb_run_name: "" # if not empty, the trainer will generate a name

# training
epoch_save_frequency: 0 # 0 = only save after the last epoch
epoch_save_path: "/workspace/spiking_workspace/out_spiking"
epoch_length_fixed: 10000 # this doesn't seem to have a purpose
grad_norm_clip: 1.0
warmup_tokens: 0
betas:
  - 0.9
  - 0.99
eps: 4e-9
num_workers: 0
lr_init: 6e-4
lr_final: 1e-5
# the mini-epoch is very short and of fixed length
#   (ctx_len * epoch_length_fixed tokens)
n_epoch: 1
# ===> batch_size must be divisible by B_GROUP_FORWARD and B_GROUP_BACKWARD in model.py
# For example, if your batch_size = 20, you can set B_GROUP_FORWARD = 4, B_GROUP_BACKWARD = 2
# If you see "CUDA out of memory", reduce it. Use GPU-Z to find the highest value for your VRAM.
batch_size: 12

# data
datafile_encoding: 'utf-8'
# datafile_encoding: 'utf-16le'

datafile_train: "/workspace/spiking_workspace/pretokenized/wikitext103/wikitext103" # in the case of mmap (.bin and .idx files) the name of both files should be the same, adn that that should be given here
datafile_valid: #"valid.txt"
datafile_test: #"test.txt"

# dataset_train_type: "mmap"
# dataset_train_type: "numpy" # ??? (not sure about this)
dataset_type_train: "mmap"
dataset_type_valid: "other"
dataset_type_test: "other"

validate: False
test: False


# debug
early_stopping: True
early_stopping_steps: 1 # how many steps should it stop after